{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.9"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_COL_NAMES = ['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth', 'Species']"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./iris.data.csv', names=CSV_COL_NAMES, header=0)\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = df[:119]\n",
    "test_data = df[119:]"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_label = train_data[train_data.columns[-1]]\n",
    "train_data = train_data.drop(train_data.columns[-1], axis=1)\n",
    "\n",
    "test_label = test_data[test_data.columns[-1]]\n",
    "test_data = test_data.drop(test_data.columns[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_std_dict = {col: {'mean':df[col].mean(), 'std': df[col].std()} for col in train_data.columns}\n",
    "\n",
    "def sl_normalize(x):\n",
    "    return (x - mean_std_dict['SepalLength']['mean'])/mean_std_dict['SepalLength']['std']\n",
    "\n",
    "def sw_normalize(x):\n",
    "    return (x - mean_std_dict['SepalWidth']['mean'])/mean_std_dict['SepalWidth']['std']\n",
    "\n",
    "def pl_normalize(x):\n",
    "    return (x - mean_std_dict['PetalLength']['mean'])/mean_std_dict['PetalLength']['std']\n",
    "\n",
    "def pw_normalize(x):\n",
    "    return (x - mean_std_dict['PetalWidth']['mean'])/mean_std_dict['PetalWidth']['std']"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_feature_cols = []\n",
    "my_feature_cols.append(tf.feature_column.numeric_column(key=CSV_COL_NAMES[0], normalizer_fn=sl_normalize))\n",
    "my_feature_cols.append(tf.feature_column.numeric_column(key=CSV_COL_NAMES[1], normalizer_fn=sw_normalize))\n",
    "my_feature_cols.append(tf.feature_column.numeric_column(key=CSV_COL_NAMES[2], normalizer_fn=pl_normalize))\n",
    "my_feature_cols.append(tf.feature_column.numeric_column(key=CSV_COL_NAMES[3], normalizer_fn=pw_normalize))"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(object):\n",
    "\n",
    "    def __init__(self, feature_cols):\n",
    "        super(Model, self).__init__()\n",
    "        self.feature_cols = feature_cols\n",
    "    \n",
    "    def __call__(self, input):\n",
    "        net = tf.feature_column.input_layer(feature_columns=self.feature_cols, features=input)\n",
    "        net = tf.layers.dense(inputs=net, units=10, activation=tf.nn.relu, name='layer_1')\n",
    "        net = tf.layers.dense(inputs=net, units=10, activation=tf.nn.relu, name='layer_2')\n",
    "        net = tf.layers.dense(inputs=net, units=3, activation=None, name='logits')\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_fn(features, labels, mode):\n",
    "\n",
    "    model = Model(my_feature_cols)\n",
    "    global_steps = tf.train.get_global_step()\n",
    "\n",
    "    logits = model(features)\n",
    "    logits = tf.cast(logits, tf.float64)\n",
    "    pred_logits = tf.argmax(logits, axis=1, output_type=tf.int64)\n",
    "    probs = tf.nn.softmax(logits)\n",
    "\n",
    "    predictions = {\n",
    "        'pred_logits': pred_logits,\n",
    "        'probabilities': probs\n",
    "    }\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(predictions=predictions, mode=mode)\n",
    "\n",
    "    with tf.name_scope('loss'):\n",
    "        error = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits, scope='loss')\n",
    "        tf.summary.scalar('loss', error)\n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        accuracy = tf.metrics.accuracy(labels=labels, predictions=pred_logits, name='acc')\n",
    "        tf.summary.scalar('accuracy',accuracy[1])\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.EVAL:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=error, eval_metric_ops={\n",
    "            'accuracy/accuracy':accuracy\n",
    "        }, evaluation_hooks=None)\n",
    "    \n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(error, global_step=global_steps)\n",
    "    train_hooks_list = []\n",
    "    train_tensor_log ={'accuracy': accuracy[1], 'loss':error, 'global_steps':global_steps}\n",
    "    train_hooks_list.append(tf.train.LoggingTensorHook(tensors=train_tensor_log, every_n_iter=100))\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=error, train_op=optimizer, training_hooks=train_hooks_list)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_label.unique().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def labels_nor(x):\n",
    "    tmp = { v:i for i,v in enumerate(df['Species'].unique().tolist())}\n",
    "    return x.apply(lambda x : tmp[x])\n",
    "\n",
    "def get_input_fn(features, labels, batch_size=32, shuffle=True, num_epoch=1000):\n",
    "    \n",
    "    def input_fn():\n",
    "        if labels is not None:\n",
    "            dataset = tf.data.Dataset.from_tensor_slices((dict(features), labels_nor(labels)))\n",
    "        else:\n",
    "            dataset = tf.data.Dataset.from_tensor_slices(dict(features))\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(1000)\n",
    "        dataset = dataset.batch(batch_size).repeat(num_epoch)\n",
    "        return dataset\n",
    "    return input_fn\n",
    "\n",
    "train_input_fn = get_input_fn(train_data, train_label)\n",
    "eval_input_fn = get_input_fn(test_data, test_label, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = tf.estimator.Estimator(model_fn=model_fn, model_dir='./Model')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(train_input_fn)\n",
    "metric = classifier.evaluate(eval_input_fn)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    " predict_ = {\n",
    "     'SepalLength': [5.1, 5.9, 6.9],\n",
    "     'SepalWidth': [3.3, 3.0, 3.1],\n",
    "     'PetalLength': [1.7, 4.2, 5.4],\n",
    "     'PetalWidth': [0.5, 1.5, 2.1]\n",
    " }\n",
    " df_test = pd.DataFrame(predict_)\n",
    " df_test_fn = get_input_fn(df_test, None, shuffle=False, num_epoch=1)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = classifier.predict(df_test_fn)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(preds)"
   ]
  }
 ]
}
